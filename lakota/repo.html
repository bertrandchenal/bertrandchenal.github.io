<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>lakota.repo API documentation</title>
<meta name="description" content="The `Repo` class manage the organisation of a storage location. It
provides creation and deletion of collections, synchronization with
remote â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>lakota.repo</code></h1>
</header>
<section id="section-intro">
<p>The <code><a title="lakota.repo.Repo" href="#lakota.repo.Repo">Repo</a></code> class manage the organisation of a storage location. It
provides creation and deletion of collections, synchronization with
remote repositories and garbage collection.</p>
<h2 id="create-repositories">Create repositories</h2>
<p>Create a <code><a title="lakota.repo.Repo" href="#lakota.repo.Repo">Repo</a></code> instance:</p>
<pre><code class="language-python"># in-memory
repo = Repo()
repo = Repo(&quot;memory://&quot;)
# From a local directory
repo = Repo('some/local/path')
repo = Repo('file://some/local/path')
# From an S3 location
repo = Repo('s3:///my_bucket')
# Use a list of uri to enable caching
repo = Repo(['memory://', 's3:///my_bucket'])
repo = Repo(['file:///tmp/local_cache', 's3:///my_bucket'])
</code></pre>
<p>S3 authentication is handled by
<a href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html" title="s3 credentials">boto3</a>. So you can either put your credentials in a
configuration files or in environment variables. Alternatively you an
pass them as url arguments:</p>
<pre><code class="language-python"># Special characters in KEY and SECRET must be escaped (for example
# with urllib.parse.quote)
pod = POD.from_uri('s3:///bucket_name?key=KEY&amp;secret=SECRET')
repo = Repo(pod=pod)
</code></pre>
<p>Similarly, you can use a compatible service through the <code>endpoint_url</code> parameter:</p>
<pre><code class="language-python">pod = POD.from_uri('s3:///bucket_name', endpoint_url='http://127.0.0.1:5300')
repo = Repo(pod=pod)
</code></pre>
<h2 id="access-collections">Access collections</h2>
<p>Create one or several collections:</p>
<pre><code class="language-python"># Define schema
schema = Schema(timestamp='int*', value='float')
# Create one collection
repo.create_collection(schema, 'my_collection')
# Create a few more
labels = ['one', 'or_more', 'labels']
repo.create_collection(schema, *labels)
</code></pre>
<p>List and instanciate collections</p>
<pre><code class="language-python">print(list(repo.ls())) # Print collections names
# Instanciate a collection
clct = repo.collection('my_collection')
# like pathlib, the `/` operator can be used
clct = repo / 'my_collection'
</code></pre>
<p>See <code><a title="lakota.collection" href="collection.html">lakota.collection</a></code> on how to manipulate collections</p>
<h2 id="garbage-collection">Garbage Collection</h2>
<p>After some times, some series can be overwritten, deleted, squashed or
merged. Sooner or later some pieces of data will get dereferenced,
those can be deleted to recover storage space. It is simply done with
the <code>gc</code> method, which returns the number of deleted files.</p>
<pre><code class="language-python">nb_hard_delete, nb_soft_delete = repo.gc()
</code></pre>
<p>Garbage collection is done in two-phases. On the first invocation,
dangling files are renamed, this is the soft-deletion. It is done by
adding the current time (returned by <code><a title="lakota.utils.hextime" href="utils.html#lakota.utils.hextime">hextime()</a></code>) as a
suffix, so <code>d3/9e/df960b6150e84bd82d5afaf2791a9b210030</code> will become
<code>d3/9e/df960b6150e84bd82d5afaf2791a9b210030.17b7d6ef91c</code>.</p>
<p>Any following read on that file will fails but it automatically
triggers a search for a similarly named file containing a suffix.</p>
<p>On the next invocation, the garbage collection will reconsider those
files with a suffix, and if the time represented by the suffix is
older than a given deadline (defined by <code>timeout</code> in
<code><a title="lakota.utils.Settings" href="utils.html#lakota.utils.Settings">Settings</a></code> ), it will be definitively deleted.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
The `Repo` class manage the organisation of a storage location. It
provides creation and deletion of collections, synchronization with
remote repositories and garbage collection.


## Create repositories

Create a `Repo` instance:
```python
# in-memory
repo = Repo()
repo = Repo(&#34;memory://&#34;)
# From a local directory
repo = Repo(&#39;some/local/path&#39;)
repo = Repo(&#39;file://some/local/path&#39;)
# From an S3 location
repo = Repo(&#39;s3:///my_bucket&#39;)
# Use a list of uri to enable caching
repo = Repo([&#39;memory://&#39;, &#39;s3:///my_bucket&#39;])
repo = Repo([&#39;file:///tmp/local_cache&#39;, &#39;s3:///my_bucket&#39;])
```

S3 authentication is handled by
[boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
&#34;s3 credentials&#34;). So you can either put your credentials in a
configuration files or in environment variables. Alternatively you an
pass them as url arguments:

```python
# Special characters in KEY and SECRET must be escaped (for example
# with urllib.parse.quote)
pod = POD.from_uri(&#39;s3:///bucket_name?key=KEY&amp;secret=SECRET&#39;)
repo = Repo(pod=pod)
```

Similarly, you can use a compatible service through the `endpoint_url` parameter:

```python
pod = POD.from_uri(&#39;s3:///bucket_name&#39;, endpoint_url=&#39;http://127.0.0.1:5300&#39;)
repo = Repo(pod=pod)
```

## Access collections

Create one or several collections:
```python
# Define schema
schema = Schema(timestamp=&#39;int*&#39;, value=&#39;float&#39;)
# Create one collection
repo.create_collection(schema, &#39;my_collection&#39;)
# Create a few more
labels = [&#39;one&#39;, &#39;or_more&#39;, &#39;labels&#39;]
repo.create_collection(schema, *labels)
```

List and instanciate collections
```python
print(list(repo.ls())) # Print collections names
# Instanciate a collection
clct = repo.collection(&#39;my_collection&#39;)
# like pathlib, the `/` operator can be used
clct = repo / &#39;my_collection&#39;
```

See `lakota.collection` on how to manipulate collections


## Garbage Collection

After some times, some series can be overwritten, deleted, squashed or
merged. Sooner or later some pieces of data will get dereferenced,
those can be deleted to recover storage space. It is simply done with
the `gc` method, which returns the number of deleted files.

```python
nb_hard_delete, nb_soft_delete = repo.gc()
```

Garbage collection is done in two-phases. On the first invocation,
dangling files are renamed, this is the soft-deletion. It is done by
adding the current time (returned by `lakota.utils.hextime`) as a
suffix, so `d3/9e/df960b6150e84bd82d5afaf2791a9b210030` will become
`d3/9e/df960b6150e84bd82d5afaf2791a9b210030.17b7d6ef91c`.

Any following read on that file will fails but it automatically
triggers a search for a similarly named file containing a suffix.

On the next invocation, the garbage collection will reconsider those
files with a suffix, and if the time represented by the suffix is
older than a given deadline (defined by `timeout` in
`lakota.utils.Settings` ), it will be definitively deleted.
&#34;&#34;&#34;

import csv
import json
from io import BytesIO, StringIO
from itertools import chain
from time import time
from uuid import uuid4

from numpy import where

from .changelog import zero_hash
from .collection import Collection
from .pod import POD
from .schema import Schema
from .utils import Pool, hashed_path, hexdigest, hextime, logger, settings

__all__ = [&#34;Repo&#34;]


class Repo:
    schema = Schema.kv(label=&#34;str*&#34;, meta=&#34;O&#34;)

    def __init__(self, uri=None, pod=None):
        &#34;&#34;&#34;
        `uri` : a string or a list of string representing a storage
        location

        `pod`
        : a `lakota.pod.POD` instance
        &#34;&#34;&#34;
        pod = pod or POD.from_uri(uri)
        folder, filename = hashed_path(zero_hash)
        self.pod = pod
        path = folder / filename
        self.registry = Collection(&#34;registry&#34;, self.schema, path, self)

    def ls(self):
        return [item.label for item in self.search()]

    def __iter__(self):
        return self.search()

    def search(self, label=None, namespace=&#34;collection&#34;):
        if label:
            start = stop = (label,)
        else:
            start = stop = None
        series = self.registry.series(namespace)
        frm = series.frame(start=start, stop=stop, closed=&#34;BOTH&#34;)
        for l in frm[&#34;label&#34;]:
            yield self.collection(l, frm)

    def __truediv__(self, name):
        return self.collection(name)

    def collection(self, label, from_frm=None, namespace=&#34;collection&#34;):
        series = self.registry.series(namespace)
        if from_frm:
            frm = from_frm.islice([label], [label], closed=&#34;BOTH&#34;)
        else:
            frm = series.frame(start=label, stop=label, closed=&#34;BOTH&#34;)

        if frm.empty:
            return None
        meta = frm[&#34;meta&#34;][-1]
        return self.reify(label, meta)

    def create_collection(
        self, schema, *labels, raise_if_exists=True, namespace=&#34;collection&#34;
    ):
        &#34;&#34;&#34;
        `schema`
        : A `lakota.schema.Schema` instance

        `labels`
        : One or more collection name

        `raise_if_exists`
        : Raise an exception if the label is already present
        &#34;&#34;&#34;
        assert isinstance(
            schema, Schema
        ), &#34;The schema parameter must be an instance of lakota.Schema&#34;
        meta = []
        schema_dump = schema.dumps()

        series = self.registry.series(namespace)
        current_labels = series.frame(
            start=min(labels), stop=max(labels), closed=&#34;BOTH&#34;, select=&#34;label&#34;
        )[&#34;label&#34;]

        new_labels = []
        for label in labels:
            label = label.strip()
            if len(label) == 0:
                raise ValueError(f&#34;Invalid label: {label}&#34;)
            if label in current_labels:
                if raise_if_exists:
                    raise ValueError(f&#34;Collection with label &#39;{label}&#39; already exists&#34;)
                else:
                    continue
            # Generate random digest
            digest = hexdigest(uuid4().bytes)
            folder, filename = hashed_path(digest)
            new_labels.append(label)
            meta.append({&#34;path&#34;: str(folder / filename), &#34;schema&#34;: schema_dump})

        if new_labels:
            series.write({&#34;label&#34;: new_labels, &#34;meta&#34;: meta})

        # Return collections
        mask = &#34;(isin self.label (list %s))&#34; % &#34; &#34;.join(f&#39;&#34;{l}&#34;&#39; for l in labels)
        frm = series.frame(start=min(labels), stop=max(labels), closed=&#34;BOTH&#34;).mask(
            mask
        )
        res = [self.collection(l, from_frm=frm) for l in labels]
        if len(labels) == 1:
            return res[0]
        return res

    def reify(self, name, meta):
        schema = Schema.loads(meta[&#34;schema&#34;])
        return Collection(name, schema, meta[&#34;path&#34;], self)

    def archive(self, collection):
        label = collection.label
        archive = self.collection(label, mode=&#34;archive&#34;)
        if archive:
            return archive
        return self.create_collection(collection.schema, label, mode=&#34;archive&#34;)

    def delete(self, *labels, namespace=&#34;collection&#34;):
        &#34;&#34;&#34;
        Delete one or more collections

        `*labels`
        : Strings, names of the collection do delete

        &#34;&#34;&#34;
        to_remove = []
        for l in labels:
            clct = self.collection(l)
            if not clct:
                continue
            to_remove.append(clct.changelog.pod)
        series = self.registry.series(namespace)
        series.delete(*labels)
        for pod in to_remove:
            try:
                pod.rm(&#34;.&#34;, recursive=True)
            except FileNotFoundError:
                continue

    def refresh(self):
        self.registry.refresh()

    def push(self, remote, *labels, shallow=False):
        &#34;&#34;&#34;
        Push local revisions (and related segments) to `remote` Repo.
        `remote`
        : A `lakota.repo.Repo` instance

        `labels`
        : The collections to push. If not given, all collections are pushed
        &#34;&#34;&#34;
        return remote.pull(self, *labels, shallow=shallow)

    def pull(self, remote, *labels, shallow=False):
        &#34;&#34;&#34;
        Pull revisions from `remote` Repo (and related segments).
        `remote`
        : A `lakota.repo.Repo` instance

        `labels`
        : The collections to pull. If not given, all collections are pulled
        &#34;&#34;&#34;

        assert isinstance(remote, Repo), &#34;A Repo instance is required&#34;
        # Pull registry
        self.registry.pull(remote.registry, shallow=shallow)
        # Extract frames
        local_cache = {l.label: l for l in self.search()}
        remote_cache = {r.label: r for r in remote.search()}
        if not labels:
            labels = remote_cache.keys()
        for label in labels:
            logger.info(&#34;Sync collection: %s&#34;, label)
            r_clct = remote_cache[label]
            if not label in local_cache:
                l_clct = self.create_collection(r_clct.schema, label)
            else:
                l_clct = local_cache[label]
                if l_clct.schema != r_clct.schema:
                    msg = (
                        f&#39;Unable to sync collection &#34;{label}&#34;,&#39;
                        &#34;incompatible meta-info.&#34;
                    )
                    raise ValueError(msg)
            l_clct.pull(r_clct, shallow=shallow)

    def merge(self):
        &#34;&#34;&#34;
        Merge repository registry. Needed when collections have been created
        or deleted concurrently.
        &#34;&#34;&#34;
        return self.registry.merge()

    def rename(self, from_label, to_label, namespace=&#34;collection&#34;):
        &#34;&#34;&#34;
        Change the label of a collection
        &#34;&#34;&#34;
        series = self.registry.series(namespace)
        frm = series.frame()
        idx = where(frm[&#34;label&#34;] == from_label)[0]
        if len(idx) == 0:
            raise ValueError(f&#39;Collection &#34;{from_label}&#34; does not exists&#39;)
        if to_label in frm[&#34;label&#34;]:
            raise ValueError(f&#39;Collection &#34;{to_label}&#34; already exists&#39;)

        # Extract bounds
        start, stop = frm.start(), frm.stop()
        # unpack idx
        idx = idx[0]
        # rebuild label list
        labels = list(frm[&#34;label&#34;])
        frm[&#34;label&#34;] = labels[:idx] + [to_label] + labels[idx + 1 :]
        # Re-order frame
        frm = frm.sorted()

        series.write(
            frm,
            # Make sure we over-write the previous content:
            start=min(frm.start(), start),
            stop=max(frm.stop(), stop),
        )

    def gc(self):
        &#34;&#34;&#34;
        Loop on all series, collect all used digests, and delete obsolete
        ones.
        &#34;&#34;&#34;
        logger.info(&#34;Start GC&#34;)
        # Collect digests across folders (_walk_folder ignore files
        # containing changelogs, it will only return segments of data)
        base_folders = self.pod.ls()
        with Pool() as pool:
            for folder in base_folders:
                pool.submit(self._walk_folder, folder)
        all_dig = set(chain(*pool.results))

        # Collect digest from changelogs. Because commits are written
        # after the segments, we minimize chance to bury data created
        # concurrently.
        self.refresh()
        active_digests = set(self.registry.digests())
        for namespace in self.registry.ls():
            for clct in self.search(namespace=namespace):
                active_digests.update(clct.digests())

        nb_hard_del = 0
        nb_soft_del = 0
        current_ts_ext = f&#34;.{hextime()}&#34;
        deadline = hextime(time() - settings.timeout)

        # Soft-Delete (&#34;bury&#34;) files on fs not in changelogs
        inactive = all_dig - active_digests
        for dig in inactive:
            if not &#34;.&#34; in dig:
                # Disable digest
                folder, filename = hashed_path(dig)
                path = str(folder / filename)
                self.pod.mv(path, path + current_ts_ext, missing_ok=True)
                nb_soft_del += 1
                continue

            # Inactive file, check timestamp &amp; delete or re-enable it
            dig, ext = dig.split(&#34;.&#34;)

            if ext &gt; deadline:
                # ext contains a ts created recently, we can not act on it yet
                continue

            folder, filename = hashed_path(dig)
            path = str(folder / filename)

            if dig in active_digests:
                # Re-enable by removing extension
                self.pod.mv(path + f&#34;.{ext}&#34;, path, missing_ok=True)
            else:
                # Permanent deletion
                self.pod.rm(path + f&#34;.{ext}&#34;, missing_ok=True)
                nb_hard_del += 1

        logger.info(
            &#34;End of GC (hard deletions: %s, soft deletions: %s)&#34;,
            nb_hard_del,
            nb_soft_del,
        )
        return nb_hard_del, nb_soft_del

    def _walk_folder(self, folder):
        &#39;&#39;&#39;
        Return list of digests contained in the top folder (ignoring
        content deeper than 2 subfolders and so ignores changelog
        files)
        &#39;&#39;&#39;
        digs = []
        pod = self.pod.cd(folder)
        for filename in pod.walk(max_depth=2):
            digs.append(folder + filename.replace(&#34;/&#34;, &#34;&#34;))

        return digs

    def import_collections(self, src, collections=None):
        &#34;&#34;&#34;
        Import collections from given `src`. It can url accepted by
        `POD.from_uri` or a `POD` instance. `collections` is the list of
        collections to load (all collection are loaded if not set).
        &#34;&#34;&#34;
        if not isinstance(src, POD):
            src = POD.from_uri(src)
        names = collections or src.ls()
        for clc_name in names:
            clc = self / clc_name
            pod = src.cd(clc_name)
            if clc is None:
                json_schema = pod.read(&#34;_schema.json&#34;).decode()
                schema = Schema.loads(json.loads(json_schema))
                clc = self.create_collection(schema, clc_name)
            logger.info(&#39;Import collection &#34;%s&#34;&#39;, clc_name)
            with clc.multi():
                for file_name in pod.ls():
                    if file_name.startswith(&#34;_&#34;):
                        continue
                    self.import_series(pod, clc, file_name)

    def import_series(self, from_pod, collection, filename):
        stem, ext = filename.rsplit(&#34;.&#34;, 1)
        column_names = sorted(collection.schema)
        if ext == &#34;csv&#34;:
            buff = StringIO(from_pod.read(filename).decode())
            reader = csv.reader(buff)
            headers = next(reader)
            assert sorted(headers) == column_names
            columns = zip(*reader)
            frm = dict(zip(headers, columns))
            srs = collection / stem
            srs.write(frm)

        elif ext == &#34;parquet&#34;:
            from pandas import read_parquet

            buff = BytesIO(from_pod.read(filename))
            df = read_parquet(buff)
            assert sorted(df.columns) == column_names
            srs = collection / stem
            srs.write(df)
        else:
            raise ValueError(f&#34;Unable to load {filename}, extension not supported&#34;)

    def export_collections(self, dest, collections=None, file_type=&#34;csv&#34;):
        if not isinstance(dest, POD):
            dest = POD.from_uri(dest)

        names = collections or self.ls()
        for clc_name in names:
            clc = self / clc_name
            if clc is None:
                logger.warn(&#39;Collection &#34;%s&#34; not found&#39;, clc_name)
            pod = dest.cd(clc_name)
            logger.info(&#39;Export collection &#34;%s&#34;&#39;, clc_name)
            schema = clc.schema.dumps()
            pod.write(&#34;_schema.json&#34;, json.dumps(schema).encode())
            for srs in clc:
                # Read series
                self.export_series(pod, srs, file_type)

    def export_series(self, pod, series, file_type):
        if file_type == &#34;csv&#34;:
            frm = series.frame()
            columns = list(frm)
            # Save series as csv in buff
            buff = StringIO()
            writer = csv.writer(buff)
            writer.writerow(columns)
            rows = zip(*(frm[c] for c in columns))
            writer.writerows(rows)
            # Write generated content in pod
            buff.seek(0)
            pod.write(f&#34;{series.label}.csv&#34;, buff.read().encode())

        elif file_type == &#34;parquet&#34;:
            df = series.df()
            data = df.to_parquet(compression=&#34;brotli&#34;)
            pod.write(f&#34;{series.label}.parquet&#34;, data)
        else:
            exit(f&#39;Unsupported file type &#34;{file_type}&#34;&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="lakota.repo.Repo"><code class="flex name class">
<span>class <span class="ident">Repo</span></span>
<span>(</span><span>uri=None, pod=None)</span>
</code></dt>
<dd>
<div class="desc"><p><code>uri</code> : a string or a list of string representing a storage
location</p>
<dl>
<dt><code>pod</code></dt>
<dd>a <code><a title="lakota.pod.POD" href="pod.html#lakota.pod.POD">POD</a></code> instance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Repo:
    schema = Schema.kv(label=&#34;str*&#34;, meta=&#34;O&#34;)

    def __init__(self, uri=None, pod=None):
        &#34;&#34;&#34;
        `uri` : a string or a list of string representing a storage
        location

        `pod`
        : a `lakota.pod.POD` instance
        &#34;&#34;&#34;
        pod = pod or POD.from_uri(uri)
        folder, filename = hashed_path(zero_hash)
        self.pod = pod
        path = folder / filename
        self.registry = Collection(&#34;registry&#34;, self.schema, path, self)

    def ls(self):
        return [item.label for item in self.search()]

    def __iter__(self):
        return self.search()

    def search(self, label=None, namespace=&#34;collection&#34;):
        if label:
            start = stop = (label,)
        else:
            start = stop = None
        series = self.registry.series(namespace)
        frm = series.frame(start=start, stop=stop, closed=&#34;BOTH&#34;)
        for l in frm[&#34;label&#34;]:
            yield self.collection(l, frm)

    def __truediv__(self, name):
        return self.collection(name)

    def collection(self, label, from_frm=None, namespace=&#34;collection&#34;):
        series = self.registry.series(namespace)
        if from_frm:
            frm = from_frm.islice([label], [label], closed=&#34;BOTH&#34;)
        else:
            frm = series.frame(start=label, stop=label, closed=&#34;BOTH&#34;)

        if frm.empty:
            return None
        meta = frm[&#34;meta&#34;][-1]
        return self.reify(label, meta)

    def create_collection(
        self, schema, *labels, raise_if_exists=True, namespace=&#34;collection&#34;
    ):
        &#34;&#34;&#34;
        `schema`
        : A `lakota.schema.Schema` instance

        `labels`
        : One or more collection name

        `raise_if_exists`
        : Raise an exception if the label is already present
        &#34;&#34;&#34;
        assert isinstance(
            schema, Schema
        ), &#34;The schema parameter must be an instance of lakota.Schema&#34;
        meta = []
        schema_dump = schema.dumps()

        series = self.registry.series(namespace)
        current_labels = series.frame(
            start=min(labels), stop=max(labels), closed=&#34;BOTH&#34;, select=&#34;label&#34;
        )[&#34;label&#34;]

        new_labels = []
        for label in labels:
            label = label.strip()
            if len(label) == 0:
                raise ValueError(f&#34;Invalid label: {label}&#34;)
            if label in current_labels:
                if raise_if_exists:
                    raise ValueError(f&#34;Collection with label &#39;{label}&#39; already exists&#34;)
                else:
                    continue
            # Generate random digest
            digest = hexdigest(uuid4().bytes)
            folder, filename = hashed_path(digest)
            new_labels.append(label)
            meta.append({&#34;path&#34;: str(folder / filename), &#34;schema&#34;: schema_dump})

        if new_labels:
            series.write({&#34;label&#34;: new_labels, &#34;meta&#34;: meta})

        # Return collections
        mask = &#34;(isin self.label (list %s))&#34; % &#34; &#34;.join(f&#39;&#34;{l}&#34;&#39; for l in labels)
        frm = series.frame(start=min(labels), stop=max(labels), closed=&#34;BOTH&#34;).mask(
            mask
        )
        res = [self.collection(l, from_frm=frm) for l in labels]
        if len(labels) == 1:
            return res[0]
        return res

    def reify(self, name, meta):
        schema = Schema.loads(meta[&#34;schema&#34;])
        return Collection(name, schema, meta[&#34;path&#34;], self)

    def archive(self, collection):
        label = collection.label
        archive = self.collection(label, mode=&#34;archive&#34;)
        if archive:
            return archive
        return self.create_collection(collection.schema, label, mode=&#34;archive&#34;)

    def delete(self, *labels, namespace=&#34;collection&#34;):
        &#34;&#34;&#34;
        Delete one or more collections

        `*labels`
        : Strings, names of the collection do delete

        &#34;&#34;&#34;
        to_remove = []
        for l in labels:
            clct = self.collection(l)
            if not clct:
                continue
            to_remove.append(clct.changelog.pod)
        series = self.registry.series(namespace)
        series.delete(*labels)
        for pod in to_remove:
            try:
                pod.rm(&#34;.&#34;, recursive=True)
            except FileNotFoundError:
                continue

    def refresh(self):
        self.registry.refresh()

    def push(self, remote, *labels, shallow=False):
        &#34;&#34;&#34;
        Push local revisions (and related segments) to `remote` Repo.
        `remote`
        : A `lakota.repo.Repo` instance

        `labels`
        : The collections to push. If not given, all collections are pushed
        &#34;&#34;&#34;
        return remote.pull(self, *labels, shallow=shallow)

    def pull(self, remote, *labels, shallow=False):
        &#34;&#34;&#34;
        Pull revisions from `remote` Repo (and related segments).
        `remote`
        : A `lakota.repo.Repo` instance

        `labels`
        : The collections to pull. If not given, all collections are pulled
        &#34;&#34;&#34;

        assert isinstance(remote, Repo), &#34;A Repo instance is required&#34;
        # Pull registry
        self.registry.pull(remote.registry, shallow=shallow)
        # Extract frames
        local_cache = {l.label: l for l in self.search()}
        remote_cache = {r.label: r for r in remote.search()}
        if not labels:
            labels = remote_cache.keys()
        for label in labels:
            logger.info(&#34;Sync collection: %s&#34;, label)
            r_clct = remote_cache[label]
            if not label in local_cache:
                l_clct = self.create_collection(r_clct.schema, label)
            else:
                l_clct = local_cache[label]
                if l_clct.schema != r_clct.schema:
                    msg = (
                        f&#39;Unable to sync collection &#34;{label}&#34;,&#39;
                        &#34;incompatible meta-info.&#34;
                    )
                    raise ValueError(msg)
            l_clct.pull(r_clct, shallow=shallow)

    def merge(self):
        &#34;&#34;&#34;
        Merge repository registry. Needed when collections have been created
        or deleted concurrently.
        &#34;&#34;&#34;
        return self.registry.merge()

    def rename(self, from_label, to_label, namespace=&#34;collection&#34;):
        &#34;&#34;&#34;
        Change the label of a collection
        &#34;&#34;&#34;
        series = self.registry.series(namespace)
        frm = series.frame()
        idx = where(frm[&#34;label&#34;] == from_label)[0]
        if len(idx) == 0:
            raise ValueError(f&#39;Collection &#34;{from_label}&#34; does not exists&#39;)
        if to_label in frm[&#34;label&#34;]:
            raise ValueError(f&#39;Collection &#34;{to_label}&#34; already exists&#39;)

        # Extract bounds
        start, stop = frm.start(), frm.stop()
        # unpack idx
        idx = idx[0]
        # rebuild label list
        labels = list(frm[&#34;label&#34;])
        frm[&#34;label&#34;] = labels[:idx] + [to_label] + labels[idx + 1 :]
        # Re-order frame
        frm = frm.sorted()

        series.write(
            frm,
            # Make sure we over-write the previous content:
            start=min(frm.start(), start),
            stop=max(frm.stop(), stop),
        )

    def gc(self):
        &#34;&#34;&#34;
        Loop on all series, collect all used digests, and delete obsolete
        ones.
        &#34;&#34;&#34;
        logger.info(&#34;Start GC&#34;)
        # Collect digests across folders (_walk_folder ignore files
        # containing changelogs, it will only return segments of data)
        base_folders = self.pod.ls()
        with Pool() as pool:
            for folder in base_folders:
                pool.submit(self._walk_folder, folder)
        all_dig = set(chain(*pool.results))

        # Collect digest from changelogs. Because commits are written
        # after the segments, we minimize chance to bury data created
        # concurrently.
        self.refresh()
        active_digests = set(self.registry.digests())
        for namespace in self.registry.ls():
            for clct in self.search(namespace=namespace):
                active_digests.update(clct.digests())

        nb_hard_del = 0
        nb_soft_del = 0
        current_ts_ext = f&#34;.{hextime()}&#34;
        deadline = hextime(time() - settings.timeout)

        # Soft-Delete (&#34;bury&#34;) files on fs not in changelogs
        inactive = all_dig - active_digests
        for dig in inactive:
            if not &#34;.&#34; in dig:
                # Disable digest
                folder, filename = hashed_path(dig)
                path = str(folder / filename)
                self.pod.mv(path, path + current_ts_ext, missing_ok=True)
                nb_soft_del += 1
                continue

            # Inactive file, check timestamp &amp; delete or re-enable it
            dig, ext = dig.split(&#34;.&#34;)

            if ext &gt; deadline:
                # ext contains a ts created recently, we can not act on it yet
                continue

            folder, filename = hashed_path(dig)
            path = str(folder / filename)

            if dig in active_digests:
                # Re-enable by removing extension
                self.pod.mv(path + f&#34;.{ext}&#34;, path, missing_ok=True)
            else:
                # Permanent deletion
                self.pod.rm(path + f&#34;.{ext}&#34;, missing_ok=True)
                nb_hard_del += 1

        logger.info(
            &#34;End of GC (hard deletions: %s, soft deletions: %s)&#34;,
            nb_hard_del,
            nb_soft_del,
        )
        return nb_hard_del, nb_soft_del

    def _walk_folder(self, folder):
        &#39;&#39;&#39;
        Return list of digests contained in the top folder (ignoring
        content deeper than 2 subfolders and so ignores changelog
        files)
        &#39;&#39;&#39;
        digs = []
        pod = self.pod.cd(folder)
        for filename in pod.walk(max_depth=2):
            digs.append(folder + filename.replace(&#34;/&#34;, &#34;&#34;))

        return digs

    def import_collections(self, src, collections=None):
        &#34;&#34;&#34;
        Import collections from given `src`. It can url accepted by
        `POD.from_uri` or a `POD` instance. `collections` is the list of
        collections to load (all collection are loaded if not set).
        &#34;&#34;&#34;
        if not isinstance(src, POD):
            src = POD.from_uri(src)
        names = collections or src.ls()
        for clc_name in names:
            clc = self / clc_name
            pod = src.cd(clc_name)
            if clc is None:
                json_schema = pod.read(&#34;_schema.json&#34;).decode()
                schema = Schema.loads(json.loads(json_schema))
                clc = self.create_collection(schema, clc_name)
            logger.info(&#39;Import collection &#34;%s&#34;&#39;, clc_name)
            with clc.multi():
                for file_name in pod.ls():
                    if file_name.startswith(&#34;_&#34;):
                        continue
                    self.import_series(pod, clc, file_name)

    def import_series(self, from_pod, collection, filename):
        stem, ext = filename.rsplit(&#34;.&#34;, 1)
        column_names = sorted(collection.schema)
        if ext == &#34;csv&#34;:
            buff = StringIO(from_pod.read(filename).decode())
            reader = csv.reader(buff)
            headers = next(reader)
            assert sorted(headers) == column_names
            columns = zip(*reader)
            frm = dict(zip(headers, columns))
            srs = collection / stem
            srs.write(frm)

        elif ext == &#34;parquet&#34;:
            from pandas import read_parquet

            buff = BytesIO(from_pod.read(filename))
            df = read_parquet(buff)
            assert sorted(df.columns) == column_names
            srs = collection / stem
            srs.write(df)
        else:
            raise ValueError(f&#34;Unable to load {filename}, extension not supported&#34;)

    def export_collections(self, dest, collections=None, file_type=&#34;csv&#34;):
        if not isinstance(dest, POD):
            dest = POD.from_uri(dest)

        names = collections or self.ls()
        for clc_name in names:
            clc = self / clc_name
            if clc is None:
                logger.warn(&#39;Collection &#34;%s&#34; not found&#39;, clc_name)
            pod = dest.cd(clc_name)
            logger.info(&#39;Export collection &#34;%s&#34;&#39;, clc_name)
            schema = clc.schema.dumps()
            pod.write(&#34;_schema.json&#34;, json.dumps(schema).encode())
            for srs in clc:
                # Read series
                self.export_series(pod, srs, file_type)

    def export_series(self, pod, series, file_type):
        if file_type == &#34;csv&#34;:
            frm = series.frame()
            columns = list(frm)
            # Save series as csv in buff
            buff = StringIO()
            writer = csv.writer(buff)
            writer.writerow(columns)
            rows = zip(*(frm[c] for c in columns))
            writer.writerows(rows)
            # Write generated content in pod
            buff.seek(0)
            pod.write(f&#34;{series.label}.csv&#34;, buff.read().encode())

        elif file_type == &#34;parquet&#34;:
            df = series.df()
            data = df.to_parquet(compression=&#34;brotli&#34;)
            pod.write(f&#34;{series.label}.parquet&#34;, data)
        else:
            exit(f&#39;Unsupported file type &#34;{file_type}&#34;&#39;)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="lakota.repo.Repo.schema"><code class="name">var <span class="ident">schema</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="lakota.repo.Repo.archive"><code class="name flex">
<span>def <span class="ident">archive</span></span>(<span>self, collection)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def archive(self, collection):
    label = collection.label
    archive = self.collection(label, mode=&#34;archive&#34;)
    if archive:
        return archive
    return self.create_collection(collection.schema, label, mode=&#34;archive&#34;)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.collection"><code class="name flex">
<span>def <span class="ident">collection</span></span>(<span>self, label, from_frm=None, namespace='collection')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collection(self, label, from_frm=None, namespace=&#34;collection&#34;):
    series = self.registry.series(namespace)
    if from_frm:
        frm = from_frm.islice([label], [label], closed=&#34;BOTH&#34;)
    else:
        frm = series.frame(start=label, stop=label, closed=&#34;BOTH&#34;)

    if frm.empty:
        return None
    meta = frm[&#34;meta&#34;][-1]
    return self.reify(label, meta)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.create_collection"><code class="name flex">
<span>def <span class="ident">create_collection</span></span>(<span>self, schema, *labels, raise_if_exists=True, namespace='collection')</span>
</code></dt>
<dd>
<div class="desc"><dl>
<dt><code>schema</code></dt>
<dd>A <code><a title="lakota.schema.Schema" href="schema.html#lakota.schema.Schema">Schema</a></code> instance</dd>
<dt><code>labels</code></dt>
<dd>One or more collection name</dd>
<dt><code>raise_if_exists</code></dt>
<dd>Raise an exception if the label is already present</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_collection(
    self, schema, *labels, raise_if_exists=True, namespace=&#34;collection&#34;
):
    &#34;&#34;&#34;
    `schema`
    : A `lakota.schema.Schema` instance

    `labels`
    : One or more collection name

    `raise_if_exists`
    : Raise an exception if the label is already present
    &#34;&#34;&#34;
    assert isinstance(
        schema, Schema
    ), &#34;The schema parameter must be an instance of lakota.Schema&#34;
    meta = []
    schema_dump = schema.dumps()

    series = self.registry.series(namespace)
    current_labels = series.frame(
        start=min(labels), stop=max(labels), closed=&#34;BOTH&#34;, select=&#34;label&#34;
    )[&#34;label&#34;]

    new_labels = []
    for label in labels:
        label = label.strip()
        if len(label) == 0:
            raise ValueError(f&#34;Invalid label: {label}&#34;)
        if label in current_labels:
            if raise_if_exists:
                raise ValueError(f&#34;Collection with label &#39;{label}&#39; already exists&#34;)
            else:
                continue
        # Generate random digest
        digest = hexdigest(uuid4().bytes)
        folder, filename = hashed_path(digest)
        new_labels.append(label)
        meta.append({&#34;path&#34;: str(folder / filename), &#34;schema&#34;: schema_dump})

    if new_labels:
        series.write({&#34;label&#34;: new_labels, &#34;meta&#34;: meta})

    # Return collections
    mask = &#34;(isin self.label (list %s))&#34; % &#34; &#34;.join(f&#39;&#34;{l}&#34;&#39; for l in labels)
    frm = series.frame(start=min(labels), stop=max(labels), closed=&#34;BOTH&#34;).mask(
        mask
    )
    res = [self.collection(l, from_frm=frm) for l in labels]
    if len(labels) == 1:
        return res[0]
    return res</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.delete"><code class="name flex">
<span>def <span class="ident">delete</span></span>(<span>self, *labels, namespace='collection')</span>
</code></dt>
<dd>
<div class="desc"><p>Delete one or more collections</p>
<dl>
<dt><code>*labels</code></dt>
<dd>Strings, names of the collection do delete</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete(self, *labels, namespace=&#34;collection&#34;):
    &#34;&#34;&#34;
    Delete one or more collections

    `*labels`
    : Strings, names of the collection do delete

    &#34;&#34;&#34;
    to_remove = []
    for l in labels:
        clct = self.collection(l)
        if not clct:
            continue
        to_remove.append(clct.changelog.pod)
    series = self.registry.series(namespace)
    series.delete(*labels)
    for pod in to_remove:
        try:
            pod.rm(&#34;.&#34;, recursive=True)
        except FileNotFoundError:
            continue</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.export_collections"><code class="name flex">
<span>def <span class="ident">export_collections</span></span>(<span>self, dest, collections=None, file_type='csv')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_collections(self, dest, collections=None, file_type=&#34;csv&#34;):
    if not isinstance(dest, POD):
        dest = POD.from_uri(dest)

    names = collections or self.ls()
    for clc_name in names:
        clc = self / clc_name
        if clc is None:
            logger.warn(&#39;Collection &#34;%s&#34; not found&#39;, clc_name)
        pod = dest.cd(clc_name)
        logger.info(&#39;Export collection &#34;%s&#34;&#39;, clc_name)
        schema = clc.schema.dumps()
        pod.write(&#34;_schema.json&#34;, json.dumps(schema).encode())
        for srs in clc:
            # Read series
            self.export_series(pod, srs, file_type)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.export_series"><code class="name flex">
<span>def <span class="ident">export_series</span></span>(<span>self, pod, series, file_type)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_series(self, pod, series, file_type):
    if file_type == &#34;csv&#34;:
        frm = series.frame()
        columns = list(frm)
        # Save series as csv in buff
        buff = StringIO()
        writer = csv.writer(buff)
        writer.writerow(columns)
        rows = zip(*(frm[c] for c in columns))
        writer.writerows(rows)
        # Write generated content in pod
        buff.seek(0)
        pod.write(f&#34;{series.label}.csv&#34;, buff.read().encode())

    elif file_type == &#34;parquet&#34;:
        df = series.df()
        data = df.to_parquet(compression=&#34;brotli&#34;)
        pod.write(f&#34;{series.label}.parquet&#34;, data)
    else:
        exit(f&#39;Unsupported file type &#34;{file_type}&#34;&#39;)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.gc"><code class="name flex">
<span>def <span class="ident">gc</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Loop on all series, collect all used digests, and delete obsolete
ones.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gc(self):
    &#34;&#34;&#34;
    Loop on all series, collect all used digests, and delete obsolete
    ones.
    &#34;&#34;&#34;
    logger.info(&#34;Start GC&#34;)
    # Collect digests across folders (_walk_folder ignore files
    # containing changelogs, it will only return segments of data)
    base_folders = self.pod.ls()
    with Pool() as pool:
        for folder in base_folders:
            pool.submit(self._walk_folder, folder)
    all_dig = set(chain(*pool.results))

    # Collect digest from changelogs. Because commits are written
    # after the segments, we minimize chance to bury data created
    # concurrently.
    self.refresh()
    active_digests = set(self.registry.digests())
    for namespace in self.registry.ls():
        for clct in self.search(namespace=namespace):
            active_digests.update(clct.digests())

    nb_hard_del = 0
    nb_soft_del = 0
    current_ts_ext = f&#34;.{hextime()}&#34;
    deadline = hextime(time() - settings.timeout)

    # Soft-Delete (&#34;bury&#34;) files on fs not in changelogs
    inactive = all_dig - active_digests
    for dig in inactive:
        if not &#34;.&#34; in dig:
            # Disable digest
            folder, filename = hashed_path(dig)
            path = str(folder / filename)
            self.pod.mv(path, path + current_ts_ext, missing_ok=True)
            nb_soft_del += 1
            continue

        # Inactive file, check timestamp &amp; delete or re-enable it
        dig, ext = dig.split(&#34;.&#34;)

        if ext &gt; deadline:
            # ext contains a ts created recently, we can not act on it yet
            continue

        folder, filename = hashed_path(dig)
        path = str(folder / filename)

        if dig in active_digests:
            # Re-enable by removing extension
            self.pod.mv(path + f&#34;.{ext}&#34;, path, missing_ok=True)
        else:
            # Permanent deletion
            self.pod.rm(path + f&#34;.{ext}&#34;, missing_ok=True)
            nb_hard_del += 1

    logger.info(
        &#34;End of GC (hard deletions: %s, soft deletions: %s)&#34;,
        nb_hard_del,
        nb_soft_del,
    )
    return nb_hard_del, nb_soft_del</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.import_collections"><code class="name flex">
<span>def <span class="ident">import_collections</span></span>(<span>self, src, collections=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Import collections from given <code>src</code>. It can url accepted by
<code>POD.from_uri</code> or a <code>POD</code> instance. <code>collections</code> is the list of
collections to load (all collection are loaded if not set).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_collections(self, src, collections=None):
    &#34;&#34;&#34;
    Import collections from given `src`. It can url accepted by
    `POD.from_uri` or a `POD` instance. `collections` is the list of
    collections to load (all collection are loaded if not set).
    &#34;&#34;&#34;
    if not isinstance(src, POD):
        src = POD.from_uri(src)
    names = collections or src.ls()
    for clc_name in names:
        clc = self / clc_name
        pod = src.cd(clc_name)
        if clc is None:
            json_schema = pod.read(&#34;_schema.json&#34;).decode()
            schema = Schema.loads(json.loads(json_schema))
            clc = self.create_collection(schema, clc_name)
        logger.info(&#39;Import collection &#34;%s&#34;&#39;, clc_name)
        with clc.multi():
            for file_name in pod.ls():
                if file_name.startswith(&#34;_&#34;):
                    continue
                self.import_series(pod, clc, file_name)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.import_series"><code class="name flex">
<span>def <span class="ident">import_series</span></span>(<span>self, from_pod, collection, filename)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_series(self, from_pod, collection, filename):
    stem, ext = filename.rsplit(&#34;.&#34;, 1)
    column_names = sorted(collection.schema)
    if ext == &#34;csv&#34;:
        buff = StringIO(from_pod.read(filename).decode())
        reader = csv.reader(buff)
        headers = next(reader)
        assert sorted(headers) == column_names
        columns = zip(*reader)
        frm = dict(zip(headers, columns))
        srs = collection / stem
        srs.write(frm)

    elif ext == &#34;parquet&#34;:
        from pandas import read_parquet

        buff = BytesIO(from_pod.read(filename))
        df = read_parquet(buff)
        assert sorted(df.columns) == column_names
        srs = collection / stem
        srs.write(df)
    else:
        raise ValueError(f&#34;Unable to load {filename}, extension not supported&#34;)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.ls"><code class="name flex">
<span>def <span class="ident">ls</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ls(self):
    return [item.label for item in self.search()]</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge repository registry. Needed when collections have been created
or deleted concurrently.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self):
    &#34;&#34;&#34;
    Merge repository registry. Needed when collections have been created
    or deleted concurrently.
    &#34;&#34;&#34;
    return self.registry.merge()</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.pull"><code class="name flex">
<span>def <span class="ident">pull</span></span>(<span>self, remote, *labels, shallow=False)</span>
</code></dt>
<dd>
<div class="desc"><dl>
<dt>Pull revisions from <code>remote</code> Repo (and related segments).</dt>
<dt><code>remote</code></dt>
<dd>A <code><a title="lakota.repo.Repo" href="#lakota.repo.Repo">Repo</a></code> instance</dd>
<dt><code>labels</code></dt>
<dd>The collections to pull. If not given, all collections are pulled</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pull(self, remote, *labels, shallow=False):
    &#34;&#34;&#34;
    Pull revisions from `remote` Repo (and related segments).
    `remote`
    : A `lakota.repo.Repo` instance

    `labels`
    : The collections to pull. If not given, all collections are pulled
    &#34;&#34;&#34;

    assert isinstance(remote, Repo), &#34;A Repo instance is required&#34;
    # Pull registry
    self.registry.pull(remote.registry, shallow=shallow)
    # Extract frames
    local_cache = {l.label: l for l in self.search()}
    remote_cache = {r.label: r for r in remote.search()}
    if not labels:
        labels = remote_cache.keys()
    for label in labels:
        logger.info(&#34;Sync collection: %s&#34;, label)
        r_clct = remote_cache[label]
        if not label in local_cache:
            l_clct = self.create_collection(r_clct.schema, label)
        else:
            l_clct = local_cache[label]
            if l_clct.schema != r_clct.schema:
                msg = (
                    f&#39;Unable to sync collection &#34;{label}&#34;,&#39;
                    &#34;incompatible meta-info.&#34;
                )
                raise ValueError(msg)
        l_clct.pull(r_clct, shallow=shallow)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.push"><code class="name flex">
<span>def <span class="ident">push</span></span>(<span>self, remote, *labels, shallow=False)</span>
</code></dt>
<dd>
<div class="desc"><dl>
<dt>Push local revisions (and related segments) to <code>remote</code> Repo.</dt>
<dt><code>remote</code></dt>
<dd>A <code><a title="lakota.repo.Repo" href="#lakota.repo.Repo">Repo</a></code> instance</dd>
<dt><code>labels</code></dt>
<dd>The collections to push. If not given, all collections are pushed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push(self, remote, *labels, shallow=False):
    &#34;&#34;&#34;
    Push local revisions (and related segments) to `remote` Repo.
    `remote`
    : A `lakota.repo.Repo` instance

    `labels`
    : The collections to push. If not given, all collections are pushed
    &#34;&#34;&#34;
    return remote.pull(self, *labels, shallow=shallow)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.refresh"><code class="name flex">
<span>def <span class="ident">refresh</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh(self):
    self.registry.refresh()</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.reify"><code class="name flex">
<span>def <span class="ident">reify</span></span>(<span>self, name, meta)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reify(self, name, meta):
    schema = Schema.loads(meta[&#34;schema&#34;])
    return Collection(name, schema, meta[&#34;path&#34;], self)</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.rename"><code class="name flex">
<span>def <span class="ident">rename</span></span>(<span>self, from_label, to_label, namespace='collection')</span>
</code></dt>
<dd>
<div class="desc"><p>Change the label of a collection</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename(self, from_label, to_label, namespace=&#34;collection&#34;):
    &#34;&#34;&#34;
    Change the label of a collection
    &#34;&#34;&#34;
    series = self.registry.series(namespace)
    frm = series.frame()
    idx = where(frm[&#34;label&#34;] == from_label)[0]
    if len(idx) == 0:
        raise ValueError(f&#39;Collection &#34;{from_label}&#34; does not exists&#39;)
    if to_label in frm[&#34;label&#34;]:
        raise ValueError(f&#39;Collection &#34;{to_label}&#34; already exists&#39;)

    # Extract bounds
    start, stop = frm.start(), frm.stop()
    # unpack idx
    idx = idx[0]
    # rebuild label list
    labels = list(frm[&#34;label&#34;])
    frm[&#34;label&#34;] = labels[:idx] + [to_label] + labels[idx + 1 :]
    # Re-order frame
    frm = frm.sorted()

    series.write(
        frm,
        # Make sure we over-write the previous content:
        start=min(frm.start(), start),
        stop=max(frm.stop(), stop),
    )</code></pre>
</details>
</dd>
<dt id="lakota.repo.Repo.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>self, label=None, namespace='collection')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def search(self, label=None, namespace=&#34;collection&#34;):
    if label:
        start = stop = (label,)
    else:
        start = stop = None
    series = self.registry.series(namespace)
    frm = series.frame(start=start, stop=stop, closed=&#34;BOTH&#34;)
    for l in frm[&#34;label&#34;]:
        yield self.collection(l, frm)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#create-repositories">Create repositories</a></li>
<li><a href="#access-collections">Access collections</a></li>
<li><a href="#garbage-collection">Garbage Collection</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="lakota" href="index.html">lakota</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="lakota.repo.Repo" href="#lakota.repo.Repo">Repo</a></code></h4>
<ul class="two-column">
<li><code><a title="lakota.repo.Repo.archive" href="#lakota.repo.Repo.archive">archive</a></code></li>
<li><code><a title="lakota.repo.Repo.collection" href="#lakota.repo.Repo.collection">collection</a></code></li>
<li><code><a title="lakota.repo.Repo.create_collection" href="#lakota.repo.Repo.create_collection">create_collection</a></code></li>
<li><code><a title="lakota.repo.Repo.delete" href="#lakota.repo.Repo.delete">delete</a></code></li>
<li><code><a title="lakota.repo.Repo.export_collections" href="#lakota.repo.Repo.export_collections">export_collections</a></code></li>
<li><code><a title="lakota.repo.Repo.export_series" href="#lakota.repo.Repo.export_series">export_series</a></code></li>
<li><code><a title="lakota.repo.Repo.gc" href="#lakota.repo.Repo.gc">gc</a></code></li>
<li><code><a title="lakota.repo.Repo.import_collections" href="#lakota.repo.Repo.import_collections">import_collections</a></code></li>
<li><code><a title="lakota.repo.Repo.import_series" href="#lakota.repo.Repo.import_series">import_series</a></code></li>
<li><code><a title="lakota.repo.Repo.ls" href="#lakota.repo.Repo.ls">ls</a></code></li>
<li><code><a title="lakota.repo.Repo.merge" href="#lakota.repo.Repo.merge">merge</a></code></li>
<li><code><a title="lakota.repo.Repo.pull" href="#lakota.repo.Repo.pull">pull</a></code></li>
<li><code><a title="lakota.repo.Repo.push" href="#lakota.repo.Repo.push">push</a></code></li>
<li><code><a title="lakota.repo.Repo.refresh" href="#lakota.repo.Repo.refresh">refresh</a></code></li>
<li><code><a title="lakota.repo.Repo.reify" href="#lakota.repo.Repo.reify">reify</a></code></li>
<li><code><a title="lakota.repo.Repo.rename" href="#lakota.repo.Repo.rename">rename</a></code></li>
<li><code><a title="lakota.repo.Repo.schema" href="#lakota.repo.Repo.schema">schema</a></code></li>
<li><code><a title="lakota.repo.Repo.search" href="#lakota.repo.Repo.search">search</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>